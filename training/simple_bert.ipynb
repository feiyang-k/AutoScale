{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "!nvidia-smi\n",
    "import platform\n",
    "platform.python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d563e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e50a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "# before execute any computation / allocation\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1288570",
   "metadata": {},
   "source": [
    "### Training a BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2665924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 99\n",
    "\n",
    "# from shutil import copyfile\n",
    "\n",
    "import os, wandb\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"xxx\"\n",
    "project_name = 'xxx'\n",
    "\n",
    "wandb.init(project=project_name, name=\"xxx\"+str(it))\n",
    "# wandb.init(project=project_name, id='uxo7savy', resume=\"must\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import pickle\n",
    "import lzma\n",
    "import random\n",
    "import gzip\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.current_device()\n",
    "\n",
    "def load_data():\n",
    "    random.seed(929)\n",
    "    \n",
    "    data_path_a = '/llm_datasets/news_full_0to80.pkl' \n",
    "    data_path_b = '/llm_datasets/amz_0to80.pkl' \n",
    "    data_path_c1 = '/llm_datasets/arxivfull1k_0to80.pkl' \n",
    "    data_path_c2 = '/llm_datasets/arxivabs1k_0to80.pkl'  \n",
    "\n",
    "    with open(data_path_a, 'rb') as file:\n",
    "        data_a = pickle.load(file)\n",
    "    data_a=data_a.tolist()\n",
    "\n",
    "    with open(data_path_b, 'rb') as file:\n",
    "        data_b = pickle.load(file)\n",
    "    # data_b=data_b.tolist()\n",
    "\n",
    "    with open(data_path_c1, 'rb') as file:\n",
    "        data_c1 = pickle.load(file)\n",
    "    data_c1=data_c1.tolist()\n",
    "\n",
    "    with open(data_path_c2, 'rb') as file:\n",
    "        data_c2 = pickle.load(file)\n",
    "    data_c2=data_c2.tolist()\n",
    "\n",
    "    # \tdata_b = pd.read_pickle(data_path_b)\n",
    "    # \tdata_c1 = pd.read_pickle(data_path_c1).tolist()\n",
    "    # \tdata_c2 = pd.read_pickle(data_path_c2).tolist()\n",
    "    data_c = data_c1 + data_c2\n",
    "\n",
    "    data_path_d = '/llm_datasets/real_books_2M.pkl' \n",
    "    data_path_e = '/llm_datasets/wiki_0to80.pkl' \n",
    "    data_path_f1 = '/llm_datasets/pubmedfull_0to80.pkl' \n",
    "    data_path_f2 = '/llm_datasets/pubmedabs_0to80.pkl'  \n",
    "\n",
    "    with open(data_path_d, 'rb') as file:\n",
    "        data_d = pickle.load(file)\n",
    "    data_d=data_d.tolist()\n",
    "\n",
    "    with open(data_path_e, 'rb') as file:\n",
    "        data_e = pickle.load(file)\n",
    "    data_e=data_e.tolist()\n",
    "\n",
    "    with open(data_path_f1, 'rb') as file:\n",
    "        data_f1 = pickle.load(file)\n",
    "    data_f1=data_f1.tolist()\n",
    "\n",
    "    with open(data_path_f2, 'rb') as file:\n",
    "        data_f2 = pickle.load(file)\n",
    "    data_f2=data_f2.tolist()\n",
    "\n",
    "    # \tdata_b = pd.read_pickle(data_path_b)\n",
    "    # \tdata_c1 = pd.read_pickle(data_path_c1).tolist()\n",
    "    # \tdata_c2 = pd.read_pickle(data_path_c2).tolist()\n",
    "    data_f = data_f1 + data_f2\n",
    "    \n",
    "    data_path_g = '/llm_datasets/github_py24.pkl' \n",
    "    data_path_o1 = '/llm_datasets/ready1k_owtc_3M.pkl'  \n",
    "    data_path_o5 = '/llm_datasets/ready500_owtc.pkl' \n",
    "\n",
    "    with open(data_path_g, 'rb') as file:\n",
    "        data_g = pickle.load(file)\n",
    "\n",
    "    with open(data_path_o1, 'rb') as file:\n",
    "        data_o1 = pickle.load(file)\n",
    "    data_o1=data_o1.tolist()\n",
    "\n",
    "    with open(data_path_o5, 'rb') as file:\n",
    "        data_o5 = pickle.load(file)\n",
    "    data_o5=data_o5.tolist()\n",
    "\n",
    "    random.shuffle(data_a)\n",
    "    random.shuffle(data_b)\n",
    "    random.shuffle(data_c)\n",
    "    random.shuffle(data_d)\n",
    "    random.shuffle(data_e)\n",
    "    random.shuffle(data_f)\n",
    "    random.shuffle(data_g)\n",
    "    random.shuffle(data_o1)\n",
    "    random.shuffle(data_o5)\n",
    "\n",
    "    number_of_samples = int(400600) # sample 1m from each domain\n",
    "    number_of_samples2 = int(180308) # sample 1m from each domain\n",
    "    data_a = random.sample(data_a, number_of_samples)\n",
    "    data_b = random.sample(data_b, number_of_samples)\n",
    "    data_c = random.sample(data_c, number_of_samples)\n",
    "\n",
    "    data_d = random.sample(data_d, number_of_samples)\n",
    "    data_e = random.sample(data_e, number_of_samples)\n",
    "    data_f = random.sample(data_f, number_of_samples)\n",
    "    \n",
    "    data_g = random.sample(data_g, number_of_samples)\n",
    "    data_o1 = random.sample(data_o1, number_of_samples)\n",
    "    data_o5 = random.sample(data_o5, number_of_samples)\n",
    "\n",
    "    return data_a, data_b, data_c, data_d, data_e, data_f, data_g, data_o1, data_o5\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"start\")\n",
    "\n",
    "    data_a, data_b, data_c, data_d, data_e, data_f, data_g, data_o1, data_o5 = load_data()\n",
    "\n",
    "    # split data into train and val\n",
    "    test_ratio = 0.002\n",
    "    test_ratio2 = 0.0015\n",
    "\n",
    "    train_data_a, val_data_a = train_test_split(data_a, test_size=test_ratio, shuffle=True, random_state=40)\n",
    "    train_data_b, val_data_b = train_test_split(data_b, test_size=test_ratio, shuffle=True, random_state=40)\n",
    "    train_data_c, val_data_c = train_test_split(data_c, test_size=test_ratio, shuffle=True, random_state=40)\n",
    "    train_data_d, val_data_d = train_test_split(data_d, test_size=test_ratio, shuffle=True, random_state=40)\n",
    "    train_data_e, val_data_e = train_test_split(data_e, test_size=test_ratio, shuffle=True, random_state=40)\n",
    "    train_data_f, val_data_f = train_test_split(data_f, test_size=test_ratio, shuffle=True, random_state=40)\n",
    "    train_data_g, val_data_g = train_test_split(data_g, test_size=test_ratio, shuffle=True, random_state=40)\n",
    "    train_data_o1, val_data_o1 = train_test_split(data_o1, test_size=test_ratio, shuffle=True, random_state=40)\n",
    "    train_data_o5, val_data_o5 = train_test_split(data_o5, test_size=test_ratio, shuffle=True, random_state=40)\n",
    "\n",
    "    # data_mlm = train_data_a + train_data_b + train_data_c1\n",
    "    data_mlm = train_data_a[:344000] + train_data_b[:285000] + train_data_c[:193000] + train_data_d[:121000] + train_data_e[:30000] + train_data_f[:245000] + train_data_g[:146000] + train_data_o1[:380000] + train_data_o5[:56000]\n",
    "    random.shuffle(data_mlm)\n",
    "    # model_name = \"bert-base-uncased\"\n",
    "    model_name = \"bert-large-uncased\"\n",
    "    \n",
    "    tokenizer_mlm = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "\n",
    "    print(\"Tokenize training data....\")\n",
    "    train_mlm = tokenizer_mlm(data_mlm, return_tensors=\"pt\", padding=True, truncation=True, max_length=295)\n",
    "    trainset = Dataset.from_dict(train_mlm)\n",
    "    # \twith open('tokens_train_mlm_nx200k.pkl', 'wb') as file:\n",
    "    # \t\tpickle.dump(train_mlm, file, protocol=4)\n",
    "\n",
    "    print(\"Tokenize validation data....\")\n",
    "    eval_mlm_a = tokenizer_mlm(val_data_a, return_tensors=\"pt\", padding=True, truncation=True, max_length=295)\n",
    "    eval_mlm_b = tokenizer_mlm(val_data_b, return_tensors=\"pt\", padding=True, truncation=True, max_length=295)\n",
    "    eval_mlm_c = tokenizer_mlm(val_data_c, return_tensors=\"pt\", padding=True, truncation=True, max_length=295)\n",
    "    eval_mlm_d = tokenizer_mlm(val_data_d, return_tensors=\"pt\", padding=True, truncation=True, max_length=295)\n",
    "    eval_mlm_e = tokenizer_mlm(val_data_e, return_tensors=\"pt\", padding=True, truncation=True, max_length=295)\n",
    "    eval_mlm_f = tokenizer_mlm(val_data_f, return_tensors=\"pt\", padding=True, truncation=True, max_length=295)\n",
    "    eval_mlm_g = tokenizer_mlm(val_data_g, return_tensors=\"pt\", padding=True, truncation=True, max_length=295)\n",
    "    eval_mlm_o1 = tokenizer_mlm(val_data_o1, return_tensors=\"pt\", padding=True, truncation=True, max_length=295)\n",
    "    eval_mlm_o5 = tokenizer_mlm(val_data_o5, return_tensors=\"pt\", padding=True, truncation=True, max_length=295)\n",
    "\n",
    "    val_a = Dataset.from_dict(eval_mlm_a)\n",
    "    val_b = Dataset.from_dict(eval_mlm_b)\n",
    "    val_c = Dataset.from_dict(eval_mlm_c)\n",
    "    val_d = Dataset.from_dict(eval_mlm_d)\n",
    "    val_e = Dataset.from_dict(eval_mlm_e)\n",
    "    val_f = Dataset.from_dict(eval_mlm_f)\n",
    "    val_g = Dataset.from_dict(eval_mlm_g)\n",
    "    val_o1 = Dataset.from_dict(eval_mlm_o1)\n",
    "    val_o5 = Dataset.from_dict(eval_mlm_o5)\n",
    "    eval_dataset_dict = {'news': val_a, 'amazon': val_b, 'arxiv': val_c, 'books': val_d, 'wiki': val_e, 'pubmed': val_f, 'github': val_g, 'owtc1k': val_o1, 'owtc500': val_o5}\n",
    "    print(trainset)\n",
    "    print(eval_dataset_dict)\n",
    "    print(\"data is ready.\")\n",
    "\n",
    "\n",
    "    #####################  Training Arguments ############\n",
    "\n",
    "    batch_size = 48\n",
    "    epochs_mlm = 1\n",
    "    # learning_rate_mlm = 1e-4\n",
    "    # learning_rate_mlm = 3e-5\n",
    "    learning_rate_mlm = 1e-4\n",
    "\n",
    "\n",
    "    data_collator_mlm = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer_mlm,\n",
    "        mlm=True,\n",
    "        mlm_probability=0.15,  # Mask 15% of tokens,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Create a training configuration for MLM fine-tuning\n",
    "    output_dir = \"./xxx-iter-\"+str(it)+\"_log\"\n",
    "    save_dir = \"./xxx-iter-\"+str(it)\n",
    "\n",
    "    training_args_mlm = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        # overwrite_output_dir=True,\n",
    "        # save_total_limit=2,\n",
    "        num_train_epochs=epochs_mlm,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        save_strategy='steps',\n",
    "        save_steps=1000,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=500,\n",
    "        logging_strategy='steps',\n",
    "        logging_steps=500,\n",
    "        learning_rate=learning_rate_mlm,\n",
    "        lr_scheduler_type='linear',\n",
    "        optim='adamw_torch',\n",
    "        report_to='wandb',\n",
    "        logging_first_step = True\n",
    "        # resume_from_checkpoint=output_dir+'/checkpoint-10000',\n",
    "    )\n",
    "\n",
    "    # Load the pre-trained BERT model for MLM fine-tuning\n",
    "    model_mlm = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
    "    # model_mlm = AutoModelForMaskedLM.from_pretrained(output_dir+'/checkpoint-10000').to(device)\n",
    "\n",
    "    print(output_dir)\n",
    "    print(save_dir)\n",
    "    print(epochs_mlm)\n",
    "\n",
    "\n",
    "    ################### Training ################\n",
    "\n",
    "    trainer_mlm = Trainer(\n",
    "        model=model_mlm,\n",
    "        args=training_args_mlm,\n",
    "        data_collator=data_collator_mlm,\n",
    "        train_dataset=trainset,\n",
    "        eval_dataset=eval_dataset_dict\n",
    "\n",
    "    )\n",
    "\n",
    "    \n",
    "    trainer_mlm.train()\n",
    "    trainer_mlm.save_model(save_dir)\n",
    "    print(\"model is saved to: \", save_dir)\n",
    "    print(\"logs are saved to:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e613fbd5",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfd6cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_val():\n",
    "    random.seed(920)\n",
    "    \n",
    "    data_path_a = '/llm_datasets/news_full_0to80.pkl' \n",
    "    data_path_b = '/llm_datasets/amz_0to80.pkl' \n",
    "    data_path_c1 = '/llm_datasets/arxivfull1k_0to80.pkl' \n",
    "    data_path_c2 = '/llm_datasets/arxivabs1k_0to80.pkl'  \n",
    "\n",
    "    with open(data_path_a, 'rb') as file:\n",
    "        data_a = pickle.load(file)\n",
    "    data_a=data_a.tolist()\n",
    "\n",
    "    with open(data_path_b, 'rb') as file:\n",
    "        data_b = pickle.load(file)\n",
    "    # data_b=data_b.tolist()\n",
    "\n",
    "    with open(data_path_c1, 'rb') as file:\n",
    "        data_c1 = pickle.load(file)\n",
    "    data_c1=data_c1.tolist()\n",
    "\n",
    "    with open(data_path_c2, 'rb') as file:\n",
    "        data_c2 = pickle.load(file)\n",
    "    data_c2=data_c2.tolist()\n",
    "\n",
    "    # \tdata_b = pd.read_pickle(data_path_b)\n",
    "    # \tdata_c1 = pd.read_pickle(data_path_c1).tolist()\n",
    "    # \tdata_c2 = pd.read_pickle(data_path_c2).tolist()\n",
    "    data_c = data_c1 + data_c2\n",
    "\n",
    "    data_path_d = '/llm_datasets/real_books_2M.pkl' \n",
    "    data_path_e = '/llm_datasets/wiki_0to80.pkl' \n",
    "    data_path_f1 = '/llm_datasets/pubmedfull_0to80.pkl' \n",
    "    data_path_f2 = '/llm_datasets/pubmedabs_0to80.pkl'  \n",
    "\n",
    "    with open(data_path_d, 'rb') as file:\n",
    "        data_d = pickle.load(file)\n",
    "    data_d=data_d.tolist()\n",
    "\n",
    "    with open(data_path_e, 'rb') as file:\n",
    "        data_e = pickle.load(file)\n",
    "    data_e=data_e.tolist()\n",
    "\n",
    "    with open(data_path_f1, 'rb') as file:\n",
    "        data_f1 = pickle.load(file)\n",
    "    data_f1=data_f1.tolist()\n",
    "\n",
    "    with open(data_path_f2, 'rb') as file:\n",
    "        data_f2 = pickle.load(file)\n",
    "    data_f2=data_f2.tolist()\n",
    "\n",
    "    # \tdata_b = pd.read_pickle(data_path_b)\n",
    "    # \tdata_c1 = pd.read_pickle(data_path_c1).tolist()\n",
    "    # \tdata_c2 = pd.read_pickle(data_path_c2).tolist()\n",
    "    data_f = data_f1 + data_f2\n",
    "    \n",
    "    data_path_g = '/llm_datasets/github_py24.pkl' \n",
    "    data_path_o1 = '/llm_datasets/ready1k_owtc_3M.pkl'  \n",
    "    data_path_o5 = '/llm_datasets/ready500_owtc.pkl' \n",
    "\n",
    "    with open(data_path_g, 'rb') as file:\n",
    "        data_g = pickle.load(file)\n",
    "\n",
    "    with open(data_path_o1, 'rb') as file:\n",
    "        data_o1 = pickle.load(file)\n",
    "    data_o1=data_o1.tolist()\n",
    "\n",
    "    with open(data_path_o5, 'rb') as file:\n",
    "        data_o5 = pickle.load(file)\n",
    "    data_o5=data_o5.tolist()\n",
    "\n",
    "    random.shuffle(data_a)\n",
    "    random.shuffle(data_b)\n",
    "    random.shuffle(data_c)\n",
    "    random.shuffle(data_d)\n",
    "    random.shuffle(data_e)\n",
    "    random.shuffle(data_f)\n",
    "    random.shuffle(data_g)\n",
    "    random.shuffle(data_o1)\n",
    "    random.shuffle(data_o5)\n",
    "\n",
    "    number_of_samples = int(60600) # sample 1m from each domain\n",
    "    number_of_samples2 = int(180308) # sample 1m from each domain\n",
    "    data_a = random.sample(data_a, number_of_samples)\n",
    "    data_b = random.sample(data_b, number_of_samples)\n",
    "    data_c = random.sample(data_c, number_of_samples)\n",
    "\n",
    "    data_d = random.sample(data_d, number_of_samples)\n",
    "    data_e = random.sample(data_e, number_of_samples)\n",
    "    data_f = random.sample(data_f, number_of_samples)\n",
    "    \n",
    "    data_g = random.sample(data_g, number_of_samples)\n",
    "    data_o1 = random.sample(data_o1, number_of_samples)\n",
    "    data_o5 = random.sample(data_o5, number_of_samples)\n",
    "\n",
    "    return data_a, data_b, data_c, data_d, data_e, data_f, data_g, data_o1, data_o5\n",
    "\n",
    "\n",
    "data_a, data_b, data_c, data_d, data_e, data_f, data_g, data_o1, data_o5 = load_data_val()\n",
    "\n",
    "# split data into train and val\n",
    "test_ratio = 0.99\n",
    "test_ratio2 = 0.0015\n",
    "\n",
    "train_data_a, val_data_a = train_test_split(data_a, test_size=test_ratio, shuffle=True, random_state=40)\n",
    "train_data_b, val_data_b = train_test_split(data_b, test_size=test_ratio, shuffle=True, random_state=40)\n",
    "train_data_c, val_data_c = train_test_split(data_c, test_size=test_ratio, shuffle=True, random_state=40)\n",
    "train_data_d, val_data_d = train_test_split(data_d, test_size=test_ratio, shuffle=True, random_state=40)\n",
    "train_data_e, val_data_e = train_test_split(data_e, test_size=test_ratio, shuffle=True, random_state=40)\n",
    "train_data_f, val_data_f = train_test_split(data_f, test_size=test_ratio, shuffle=True, random_state=40)\n",
    "train_data_g, val_data_g = train_test_split(data_g, test_size=test_ratio, shuffle=True, random_state=40)\n",
    "train_data_o1, val_data_o1 = train_test_split(data_o1, test_size=test_ratio, shuffle=True, random_state=40)\n",
    "train_data_o5, val_data_o5 = train_test_split(data_o5, test_size=test_ratio, shuffle=True, random_state=40)\n",
    "\n",
    "\n",
    "model_name = \"bert-large-uncased\"\n",
    "tokenizer_mlm = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "\n",
    "\n",
    "print(\"Tokenize validation data....\")\n",
    "eval_mlm_a = tokenizer_mlm(val_data_a[:50000], return_tensors=\"pt\", padding=True, truncation=True, max_length=295)\n",
    "eval_mlm_b = tokenizer_mlm(val_data_b[:50000], return_tensors=\"pt\", padding=True, truncation=True, max_length=295)\n",
    "eval_mlm_c = tokenizer_mlm(val_data_c[:50000], return_tensors=\"pt\", padding=True, truncation=True, max_length=295)\n",
    "eval_mlm_d = tokenizer_mlm(val_data_d[:50000], return_tensors=\"pt\", padding=True, truncation=True, max_length=295)\n",
    "eval_mlm_e = tokenizer_mlm(val_data_e[:50000], return_tensors=\"pt\", padding=True, truncation=True, max_length=295)\n",
    "eval_mlm_f = tokenizer_mlm(val_data_f[:50000], return_tensors=\"pt\", padding=True, truncation=True, max_length=295)\n",
    "eval_mlm_g = tokenizer_mlm(val_data_g[:50000], return_tensors=\"pt\", padding=True, truncation=True, max_length=295)\n",
    "eval_mlm_o1 = tokenizer_mlm(val_data_o1[:50000], return_tensors=\"pt\", padding=True, truncation=True, max_length=295)\n",
    "eval_mlm_o5 = tokenizer_mlm(val_data_o5[:50000], return_tensors=\"pt\", padding=True, truncation=True, max_length=295)\n",
    "\n",
    "val_a = Dataset.from_dict(eval_mlm_a)\n",
    "val_b = Dataset.from_dict(eval_mlm_b)\n",
    "val_c = Dataset.from_dict(eval_mlm_c)\n",
    "val_d = Dataset.from_dict(eval_mlm_d)\n",
    "val_e = Dataset.from_dict(eval_mlm_e)\n",
    "val_f = Dataset.from_dict(eval_mlm_f)\n",
    "val_g = Dataset.from_dict(eval_mlm_g)\n",
    "val_o1 = Dataset.from_dict(eval_mlm_o1)\n",
    "val_o5 = Dataset.from_dict(eval_mlm_o5)\n",
    "eval_dataset_dict = {'news': val_a, 'amazon': val_b, 'arxiv': val_c, 'books': val_d, 'wiki': val_e, 'pubmed': val_f, 'github': val_g, 'owtc1k': val_o1, 'owtc500': val_o5}\n",
    "print(trainset)\n",
    "print(eval_dataset_dict)\n",
    "print(\"data is ready.\")\n",
    "\n",
    "# trainer_mlm.eval_dataset = eval_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b63d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_dict_s = eval_dataset_dict\n",
    "\n",
    "\n",
    "rest2 = np.zeros([10,1])\n",
    "i = 0\n",
    "for name, dataset in eval_dataset_dict_s.items():\n",
    "    print(f\"Running evaluation on {name}\")\n",
    "    trainer_mlm.eval_dataset = dataset\n",
    "    result = trainer_mlm.evaluate()\n",
    "    rest2[i,0] = np.exp(result['eval_loss'])\n",
    "    i += 1\n",
    "    print(f\"Results for {name}: {result}\")\n",
    "\n",
    "rest2[9,0] = np.average(rest2[:9,0])\n",
    "rest2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67181867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:p39h]",
   "language": "python",
   "name": "conda-env-p39h-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
